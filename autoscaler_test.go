package lawbench

import (
	"math"
	"testing"
)

func TestShouldScale_Underutilized(t *testing.T) {
	metrics := AutoScalerMetrics{
		R:        1.2,
		CurrentN: 10,
		Alpha:    0.05,
		Beta:     0.01,
		TargetR:  2.0,
	}

	rec := ShouldScale(metrics)

	if rec.Decision != ScaleDown {
		t.Errorf("Expected ScaleDown, got %v", rec.Decision)
	}

	if rec.TargetN >= metrics.CurrentN {
		t.Errorf("Expected target < current (%d), got %d", metrics.CurrentN, rec.TargetN)
	}

	if rec.CostSavings <= 0 {
		t.Errorf("Expected cost savings > 0, got %.2f%%", rec.CostSavings)
	}

	t.Logf("âœ“ Underutilized: Scale from %d to %d nodes (%.0f%% cost savings)",
		metrics.CurrentN, rec.TargetN, rec.CostSavings)
	t.Logf("  Reason: %s", rec.Reason)
}

func TestShouldScale_OptimalZone(t *testing.T) {
	metrics := AutoScalerMetrics{
		R:        2.0,
		CurrentN: 10,
		Alpha:    0.05,
		Beta:     0.01,
		TargetR:  2.0,
	}

	rec := ShouldScale(metrics)

	if rec.Decision != Maintain {
		t.Errorf("Expected Maintain, got %v", rec.Decision)
	}

	if rec.TargetN != metrics.CurrentN {
		t.Errorf("Expected target = current (%d), got %d", metrics.CurrentN, rec.TargetN)
	}

	t.Logf("âœ“ Optimal Zone: r=%.1f, maintain %d nodes", metrics.R, metrics.CurrentN)
	t.Logf("  Reason: %s", rec.Reason)
}

func TestShouldScale_StressWithHeadroom(t *testing.T) {
	metrics := AutoScalerMetrics{
		R:        2.8,
		CurrentN: 5,
		Alpha:    0.05,
		Beta:     0.01,
		Lambda:   1000,
		TargetR:  2.0,
	}

	rec := ShouldScale(metrics)

	if rec.Decision != ScaleUp {
		t.Errorf("Expected ScaleUp, got %v", rec.Decision)
	}

	if rec.InRetrograde {
		t.Errorf("Should not be in retrograde with N=5 (peak=%.1f)", rec.PeakN)
	}

	if rec.TargetN <= metrics.CurrentN {
		t.Errorf("Expected target > current (%d), got %d", metrics.CurrentN, rec.TargetN)
	}

	t.Logf("âœ“ Stress + Headroom: Scale from %d to %d nodes (peak capacity: %.1f)",
		metrics.CurrentN, rec.TargetN, rec.PeakN)
	t.Logf("  Reason: %s", rec.Reason)
}

func TestShouldScale_RetrogradeZone(t *testing.T) {
	// High Î² system: Strong coherency penalty
	metrics := AutoScalerMetrics{
		R:        2.9,
		CurrentN: 50,
		Alpha:    0.05,
		Beta:     0.02, // High Î²
		Lambda:   1000,
		TargetR:  2.0,
	}

	rec := ShouldScale(metrics)

	// Peak capacity: sqrt((1-0.05)/0.02) = sqrt(47.5) â‰ˆ 6.9
	expectedPeak := math.Sqrt((1 - metrics.Alpha) / metrics.Beta)

	if !rec.InRetrograde {
		t.Errorf("Expected retrograde with N=%d (peak=%.1f)", metrics.CurrentN, rec.PeakN)
	}

	if rec.Decision != ShedLoad {
		t.Errorf("Expected ShedLoad in retrograde, got %v", rec.Decision)
	}

	if math.Abs(rec.PeakN-expectedPeak) > 0.1 {
		t.Errorf("Peak capacity mismatch: expected %.1f, got %.1f", expectedPeak, rec.PeakN)
	}

	t.Logf("âœ“ Retrograde Zone: N=%d > N_peak=%.1f", metrics.CurrentN, rec.PeakN)
	t.Logf("  Decision: %v (DON'T add nodes)", rec.Decision)
	t.Logf("  Reason: %s", rec.Reason)
}

func TestShouldScale_ChaosMode(t *testing.T) {
	metrics := AutoScalerMetrics{
		R:        3.5,
		CurrentN: 20,
		Alpha:    0.05,
		Beta:     0.01,
		TargetR:  2.0,
	}

	rec := ShouldScale(metrics)

	if rec.Decision != ShedLoad {
		t.Errorf("Expected ShedLoad in chaos, got %v", rec.Decision)
	}

	if rec.RiskLevel != "HIGH" {
		t.Errorf("Expected HIGH risk, got %s", rec.RiskLevel)
	}

	t.Logf("âœ“ Chaos Mode: r=%.1f â‰¥ 3.0", metrics.R)
	t.Logf("  Decision: %v", rec.Decision)
	t.Logf("  Risk: %s", rec.RiskLevel)
	t.Logf("  Reason: %s", rec.Reason)
}

func TestShouldScale_EmergencyCritical(t *testing.T) {
	metrics := AutoScalerMetrics{
		R:        4.2,
		CurrentN: 20,
		Alpha:    0.05,
		Beta:     0.01,
		TargetR:  2.0,
	}

	rec := ShouldScale(metrics)

	if rec.Decision != EmergencyStop {
		t.Errorf("Expected EmergencyStop, got %v", rec.Decision)
	}

	if rec.RiskLevel != "CRITICAL" {
		t.Errorf("Expected CRITICAL risk, got %s", rec.RiskLevel)
	}

	if rec.TargetN != metrics.CurrentN {
		t.Errorf("Emergency should not change node count: expected %d, got %d",
			metrics.CurrentN, rec.TargetN)
	}

	t.Logf("âœ“ Emergency: r=%.1f â‰¥ 4.0 (full chaos)", metrics.R)
	t.Logf("  Decision: %v (DO NOT SCALE)", rec.Decision)
	t.Logf("  Risk: %s", rec.RiskLevel)
}

func TestCalculatePeakCapacity(t *testing.T) {
	tests := []struct {
		name         string
		alpha, beta  float64
		expectedPeak float64
		expectInf    bool
	}{
		{
			name:         "Low contention, low coherency",
			alpha:        0.05,
			beta:         0.01,
			expectedPeak: math.Sqrt(0.95 / 0.01), // â‰ˆ 9.75
		},
		{
			name:         "High coherency",
			alpha:        0.05,
			beta:         0.05,
			expectedPeak: math.Sqrt(0.95 / 0.05), // â‰ˆ 4.36
		},
		{
			name:      "Zero coherency (linear scaling)",
			alpha:     0.05,
			beta:      0,
			expectInf: true,
		},
		{
			name:         "Deadlock system",
			alpha:        1.0,
			beta:         0.01,
			expectedPeak: 0,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			peak := CalculatePeakCapacity(tt.alpha, tt.beta)

			if tt.expectInf {
				if !math.IsInf(peak, 1) {
					t.Errorf("Expected infinity, got %.2f", peak)
				}
				t.Logf("âœ“ %s: N_peak = âˆž (linear scaling)", tt.name)
			} else {
				if math.Abs(peak-tt.expectedPeak) > 0.01 {
					t.Errorf("Expected peak â‰ˆ %.2f, got %.2f", tt.expectedPeak, peak)
				}
				t.Logf("âœ“ %s: N_peak = %.2f", tt.name, peak)
			}
		})
	}
}

func TestEstimateThroughput(t *testing.T) {
	lambda := 1000.0
	alpha := 0.05
	beta := 0.01

	tests := []struct {
		N             int
		expectedRange [2]float64 // [min, max]
	}{
		{N: 1, expectedRange: [2]float64{1000, 1000}},  // Baseline
		{N: 2, expectedRange: [2]float64{1850, 1900}},  // Slight overhead
		{N: 4, expectedRange: [2]float64{3100, 3200}},  // Contention visible
		{N: 8, expectedRange: [2]float64{4100, 4300}},  // Approaching peak
		{N: 10, expectedRange: [2]float64{4200, 4300}}, // Near peak capacity
	}

	for _, tt := range tests {
		throughput := EstimateThroughput(tt.N, lambda, alpha, beta)

		if throughput < tt.expectedRange[0] || throughput > tt.expectedRange[1] {
			t.Errorf("N=%d: throughput %.0f outside expected range [%.0f, %.0f]",
				tt.N, throughput, tt.expectedRange[0], tt.expectedRange[1])
		}

		efficiency := throughput / (lambda * float64(tt.N)) * 100
		t.Logf("N=%2d: throughput=%.0f ops/sec (%.1f%% efficiency)",
			tt.N, throughput, efficiency)
	}
}

func TestIsRetrograde(t *testing.T) {
	tests := []struct {
		name             string
		currentN         int
		alpha, beta      float64
		expectRetrograde bool
	}{
		{
			name:             "Below peak",
			currentN:         5,
			alpha:            0.05,
			beta:             0.01,
			expectRetrograde: false,
		},
		{
			name:             "At peak",
			currentN:         10,
			alpha:            0.05,
			beta:             0.01,
			expectRetrograde: true, // sqrt(0.95/0.01) â‰ˆ 9.75
		},
		{
			name:             "Far beyond peak",
			currentN:         50,
			alpha:            0.05,
			beta:             0.02,
			expectRetrograde: true, // sqrt(0.95/0.02) â‰ˆ 6.9
		},
		{
			name:             "Zero beta (no retrograde possible)",
			currentN:         100,
			alpha:            0.05,
			beta:             0,
			expectRetrograde: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			retrograde := IsRetrograde(tt.currentN, tt.alpha, tt.beta)

			if retrograde != tt.expectRetrograde {
				t.Errorf("Expected retrograde=%v, got %v", tt.expectRetrograde, retrograde)
			}

			peak := CalculatePeakCapacity(tt.alpha, tt.beta)
			status := "âœ“"
			if retrograde {
				status = "âš ï¸"
			}
			t.Logf("%s N=%d, N_peak=%.1f, retrograde=%v",
				status, tt.currentN, peak, retrograde)
		})
	}
}

func TestKubernetesHPATarget(t *testing.T) {
	tests := []struct {
		name            string
		currentReplicas int
		currentR        float64
		targetR         float64
		alpha, beta     float64
		expectChange    bool
		expectDirection string // "up", "down", "maintain"
	}{
		{
			name:            "Underutilized - scale down",
			currentReplicas: 10,
			currentR:        1.2,
			targetR:         2.0,
			alpha:           0.05,
			beta:            0.01,
			expectChange:    true,
			expectDirection: "down",
		},
		{
			name:            "Optimal - maintain",
			currentReplicas: 10,
			currentR:        2.0,
			targetR:         2.0,
			alpha:           0.05,
			beta:            0.01,
			expectChange:    false,
			expectDirection: "maintain",
		},
		{
			name:            "Stressed - scale up",
			currentReplicas: 5,
			currentR:        2.8,
			targetR:         2.0,
			alpha:           0.05,
			beta:            0.01,
			expectChange:    true,
			expectDirection: "up",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			targetReplicas := KubernetesHPATarget(
				tt.currentReplicas,
				tt.currentR,
				tt.targetR,
				tt.alpha,
				tt.beta,
			)

			changed := targetReplicas != tt.currentReplicas

			if changed != tt.expectChange {
				t.Errorf("Expected change=%v, got %v (current=%d, target=%d)",
					tt.expectChange, changed, tt.currentReplicas, targetReplicas)
			}

			var actualDirection string
			if targetReplicas > tt.currentReplicas {
				actualDirection = "up"
			} else if targetReplicas < tt.currentReplicas {
				actualDirection = "down"
			} else {
				actualDirection = "maintain"
			}

			if actualDirection != tt.expectDirection {
				t.Errorf("Expected direction=%s, got %s", tt.expectDirection, actualDirection)
			}

			t.Logf("âœ“ %s: %d â†’ %d replicas (r=%.1f â†’ target=%.1f)",
				tt.name, tt.currentReplicas, targetReplicas, tt.currentR, tt.targetR)
		})
	}
}

func TestBillionDollarOptimization(t *testing.T) {
	t.Log("=== THE BILLION DOLLAR OPTIMIZATION ===")
	t.Log("")
	t.Log("Scenario: Your database locks up. App servers spin at 100% CPU.")
	t.Log("")

	// System with high contention (database locks)
	metrics := AutoScalerMetrics{
		R:        3.2,  // System in chaos
		CurrentN: 50,   // Already have 50 nodes
		Alpha:    0.3,  // High contention (database locks)
		Beta:     0.05, // Moderate coherency overhead
		Lambda:   1000,
		TargetR:  2.0,
	}

	// Calculate peak capacity
	peakN := CalculatePeakCapacity(metrics.Alpha, metrics.Beta)

	t.Logf("System State:")
	t.Logf("  Current nodes: %d", metrics.CurrentN)
	t.Logf("  Peak capacity: %.1f nodes", peakN)
	t.Logf("  Current r: %.1f (CHAOS)", metrics.R)
	t.Logf("  In retrograde: %v", metrics.CurrentN > int(peakN))
	t.Log("")

	// Traditional CPU-based autoscaler decision
	t.Log("Traditional Autoscaler (CPU-based):")
	t.Log("  Sees: CPU at 100%")
	t.Log("  Decides: Add 50 more nodes (scale to 100)")
	t.Log("  Cost: 2x cloud bill")
	t.Log("  Result: Î² overhead EXPLODES (NÂ² growth)")
	t.Log("  Outcome: ðŸ’¥ SYSTEM COLLAPSES FASTER")
	t.Log("  You paid Amazon extra money to kill your service")
	t.Log("")

	// lawbench decision
	rec := ShouldScale(metrics)

	t.Log("lawbench Autoscaler (r-based):")
	t.Logf("  Sees: r=%.1f (chaos), N > N_peak (retrograde)", metrics.R)
	t.Logf("  Decides: %s", rec.Decision)
	t.Logf("  Target nodes: %d (not %d!)", rec.TargetN, 100)
	t.Log("  Cost: $0 wasted on useless nodes")
	t.Log("  Result: Shed 30% of load, system stabilizes")
	t.Log("  Outcome: âœ… TOP 70% GET PERFECT SERVICE")
	t.Log("")
	t.Logf("  Reason: %s", rec.Reason)
	t.Log("")

	// Cost comparison
	traditionalNodes := 100
	lawbenchNodes := rec.TargetN
	costPerNode := 100.0 // $100/month per node

	traditionalCost := float64(traditionalNodes) * costPerNode
	lawbenchCost := float64(lawbenchNodes) * costPerNode
	savings := traditionalCost - lawbenchCost

	t.Log("ðŸ’° COST COMPARISON:")
	t.Logf("  Traditional: %d nodes Ã— $%.0f = $%.0f/month",
		traditionalNodes, costPerNode, traditionalCost)
	t.Logf("  lawbench: %d nodes Ã— $%.0f = $%.0f/month",
		lawbenchNodes, costPerNode, lawbenchCost)
	t.Logf("  Savings: $%.0f/month", savings)
	t.Log("")

	if rec.Decision != ShedLoad {
		t.Errorf("Expected ShedLoad in retrograde chaos, got %v", rec.Decision)
	}

	if rec.TargetN >= 100 {
		t.Errorf("lawbench should NOT recommend scaling to 100+ nodes in retrograde")
	}

	t.Log("âœ“ The Billion Dollar Optimization: VALIDATED")
	t.Log("  Don't add nodes when N > N_peak")
	t.Log("  Shed load instead of throwing money at the problem")
}
